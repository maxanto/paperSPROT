\section{Analysis tools}
\label{sec:quanti}

The normalized Shannon entropy applied to two different PDFs and the maximum Lyapunov exponent along with the mean period's lengths are the quantifiers employed here to estimate the system's properties. The entropies   help us to evaluate the two properties that determine the randomness degree, the equiprobability among all possible values and the statistical independence between consecutive values, while the MLE determines the presence of chaos. 
 
\subsection{Period analysis}

As said in section \ref{sec:estudio}, the maximum reachable period is $T_{max}=2^{D.n}$. Actually, the periods obtained are much lower than the maximum and are heavily dependent on the IC.

We have developed a C code that emulates the operation in a digital environment, it will be described in detail in section \ref{sec:repre}. One task of this code is to analyze the reached period when starting iteration from each initial condition with a certain number of bits. The initial condition could converge to a limit cycle, or it could be one value of the limit cycle itself. Basically, the code iterates every IC and detects when any value of the generated sequence is repeated, then it stores the period the limit cycle has.
This procedure was repeated for all the initial
conditions to obtain the attraction domain scheme of the
system.


With the developed code, we have systematically studied the behavior of the system's output
using different precisions in a fixed-point architecture.\subsection{Quantifiers of Randomness}
\label{cu_ran}

Another important characteristic that varies with the precision employed is the randomness of the sequences generated by the chaotic system. 

Based on results of previous research \cite{DeMicco2008,Antonelli2016,DeMicco2011} the normalized Shannon entropy was adopted as quantifier to characterize determinism and stochasticity of the generated sequences. This quantifier derives from the Information Theory, and it is a functional of the PDF. By a proper selection of the used PDF it is possible to cover the two mentioned properties, namely, (1) the probability of occurrence of each element of the alphabet (PDF based on histograms), and (2) the order of the items in the time series (PDF based on Bandt-Pompe technique).
A discussion about the convenience of using these quantifiers is beyond the scope of this chapter but there is an extensive literature \cite{Rosso2007A,DeMicco2008,Martin2006}.

Once the PDF is determined the entropy is defined by the very well known normalized Shannon expression:

\begin{equation}
H=\frac{\sum_{i=1}^{M}{p_i~log~p_i}}{log(M)}, \label{eq:shannon}
\end{equation}

Where $M$ is the number of elements of the alphabet.
\subsubsection{Defining the PDF:}

From a statistics point of view a chaotic system is the \textsl{source} of a symbolic time series with an alphabet of $M$ symbols. Entropy is a basic concept in information theory. To evaluate entropy  one needs first to define a probability distribution function of the time series. There is not a unique procedure to obtain this PDF and the determination of the best PDF $P$ is a fundamental problem because $P$ and the sample space are inextricably linked.
Several methods deserve mention: 
\begin{enumerate}
\item frequency counting \cite{Rosso2009}, 
\item procedures based on amplitude statistics \cite{DeMicco2008}, 
\item binary symbolic dynamics \cite{Mischaikow1999}, 
\item Fourier analysis \cite{Powell1979}, 
\item wavelet transform \cite{Rosso2001} and,
\item ordering patterns \cite{Pompe2002}, among others.  
\end{enumerate}
Their applicability depends on particular characteristics of the data, such as stationarity, time series length, variation of the parameters, level of noise contamination, etc.

Basically one may consider the statistics of individual symbols or the statistics of sequences of $D$ consecutive symbols. In the first case  $P$ is \emph{non-causal} because it does not change if the outcomes are mixed up and the number of different possible outcomes is $M$ (the number of symbols in the source alphabet). In the second case, the outcome changes if the output is mixed and then one says that $P$ is \emph{causal}. In this second case the number of different outcomes is equal to $M^D$ and increases rapidly with $D$. Bandt and Pompe made a proposal in \cite{Pompe2002} that is computationally efficient, because it limits the outcomes to $D!$, but retains causal effects. In previous works devoted to PRNGs, the use of two
PDFs was successful for the comparison between different systems. One PDF is the
normalized histogram, and its normalized Shannon
entropy is denoted here $H_{hist}$. The other one is the ordering PDF
proposed by Bandt \& Pompe \cite{Pompe2002} and its  normalized Shannon entropy is  here denoted as $H_{BP}$. Let us summarize how these
PDFs are obtained.

The representation plane $H_{BP}$ vs $H_{hist}$ is considered \cite{DeMicco2008}.
A higher value in any of the entropies, $H_{BP}$ and $H_{hist}$, implies an
increase in the uniformity of the involved PDFs. The point
$(1,1)$ represents the ideal point for a system with uniform histogram and
uniform distribution of ordering patterns.
\subsubsection{PDF based on histograms}

To evaluate the probability of occurrence of each element of the alphabet, it is possible to use the normalized histogram of the time series as a PDF.

If $Y$ is the time series being analysed $Y=\{y_i,i=1,...N\}$. The obvious PDF to characterize $Y$ is the normalized histogram of the $M$ words $Y$; let us call it  $PDF_{hist}$.
 
In order to  extract a PDF via amplitude-statistics, divide first
the interval $[0,1]$ into a finite number $nbin$ of non
overlapping subintervals $A_i$: $[0,1]=\bigcup_{i=1}^{nbin} A_i$
and $A_i\bigcap A_j=\emptyset~\forall i\neq j$. One then employs
the usual histogram-method, based on counting the relative
frequencies of the time series values within each subinterval. It
should be clear that the resulting  PDF lacks any information
regarding temporal evolution. The only pieces of information we
have here are the $y_i$-values that allow one to assign inclusion
within a given bin, ignoring just where they are located (this is,
the subindex $i$.)

\subsubsection{PDF based on Band and Pompe methodology}
%
Let $y$ be the source output and let $y_1$ to $y_N$ be a $N$-length digital time series.
To use the Bandt and Pompe \cite{Pompe2002} methodology for evaluating
of probability distribution $P$ one starts by  considering
a vector of length $D$ given by: 
\begin{equation}
(s)~\mapsto~
\left(~y_{s-(D-1)},~y_{s-(D-2)},~\dots,~y_{s-1},~y_{s}~\right) \,
 \label{eq:vectores}
\end{equation}

which assign to each time $s$ the $D$-dimensional vector of values
at times $s, s-1,\dots,s-(D-1)$. Clearly, the greater the
$D$-value, the more information on the past  is incorporated into
our vectors. By the ``ordinal pattern" related to the time $(s)$
we mean the permutation $\pi=(r_0,r_1, \dots,r_{D-1})$ of
$(0,1,\dots,D-1)$ defined by
\begin{equation}
y_{s-r_{D-1}}~\le~y_{s-r_{D-2}}~\le~\dots~\le~y_{s-r_{1}}~\le~y_{s-r_0}
\ . \label{eq:permuta}
\end{equation}
In order to get a unique result we set $r_i <r_{i-1}$ if
$y_{s-r_{i}}=y_{s-r_{i-1}}$. Thus, for all the $D!$ possible
permutations $\pi$ of order $D$, the probability distribution
$P=\{p(\pi)\}$ is defined by
\begin{equation}
p(\pi)~=~\frac{\# \{s|s\leq N-D+1;~ (s) \texttt{, has type
}\pi\}}{N-D+1} \ . \label{eq:frequ}
\end{equation}
In this expression, the symbol $\#$ stands for ``number".

The Bandt-Pompe's methodology is not restricted to time series
representative of low dimensional dynamical systems but can be
applied to any type of time series (regular, chaotic, noisy, or
reality based), with a very weak stationary assumption (for $k =
D$, the probability for $y_t < y_{t+k}$ should not depend on $t$
\cite{Pompe2002}). One also assumes that enough data are available for
a correct attractor-reconstruction. Of course, the embedding
dimension $D$ plays an important role in the evaluation of the
appropriate probability distribution because $D$ determines the
number of accessible states $D!$. Also, it conditions the minimum
acceptable length $N \gg D!$ of the time series that one needs in
order to work with a reliable statistics.


\subsection{Maximum Lyapunov Exponent}

The fourth quantifier employed is the Maximum Lyapunov Exponent that determines the presence of chaos.
The Lyapunov exponents are quantifiers that characterize how the
separation between two trajectories evolves, \cite{Sprott2003}. It
is well known that chaotic behaviors are characterized
mainly by Lyapunov numbers of the dynamic systems. If one or more
Lyapunov numbers are greater than zero, then the system behaves
chaotically. Otherwise, the system is stable. In this paper, we
employ the maximum Lyapunov number as it is one of the most useful
indicators of chaos.

The distance between trajectories changes in $2^{MLE}$ for each
iteration, on average. If $MLE<0$ the trajectories approaches,
this may be due to a fixed-point, if $MLE=0$ the trajectories keep
their distance, this may be due to a limit cycle, if $MLE>0$, the
distance between trajectories is growing, and is an indicator of
chaos, \cite{Sprott2003}.

There is a non-analytical way to measure it if only the inputs and
outputs of the system are accessible. The procedure is the
following: the system must be started from two neighbor points in
the phase plane, lets call them $(x_a,y_a)$ and $(x_b,y_b)$, as
the system is iterated the Euclidean distance between the two
trajectories is measured ($d_n$ in the $n_{th}$ sample) (eq.
\ref{eq:D0D1}), and the b trajectory is relocalized on each
iteration  (eq. \ref{eq:reubicacion}), obtaining the points
$(x_{br},y_{br})$ to feed the system. Then the Lyapunov exponent
can be calculated as shown in eq. (\ref{eq:Lyapunov}).

\begin{eqnarray}\label{eq:Lyapunov}
    MLE &=& \frac{1}{N} \sum_{i=2}^{N} \log_2{\frac{d_{1(i)}}{d_{0(i-1)}}}
\end{eqnarray}
\begin{eqnarray}\label{eq:D0D1}
    d_{0(i-1)}&=& \sqrt{(x_{a(i-1)}-x_{br(i-1)})^2+(y_{a(i-1)}-y_{br(i-1)})^2}\nonumber\\
    d_{1(i)}&=& \sqrt{(x_{a(i)}-x_{b(i)})^2+(y_{a(i)}-y_{b(i)})^2}\\
\nonumber
\end{eqnarray}
\begin{eqnarray}\label{eq:reubicacion}
    x_{br(i)}&=& x_{a(i)}+(x_{b(i)}-x_{a(i)})d_{o(i-1)}/d_{1(i)} \nonumber\\
    y_{br(i)}&=& y_{a(i)}+(y_{b(i)}-y_{a(i)})d_{o(i-1)}/d_{1(i)}
\end{eqnarray}


